---
title: "Problem Set 7"
author: "Pete Cuppernull"
date: "3/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggfortify)
library(tsne)
```

K-Means by hand
1. Initialize process, assign random cluster
```{r}
input_1 <- c(5,8,7,8,3,4,2,3,4,5)
input_2 <- c(8,6,5,4,3,2,2,8,9,8)

plot(input_1, input_2)

#create assignments
label <- as.factor(sample(1:3, 10, replace=TRUE))

#assign labels to obs
x <- cbind(input_1, input_2, label)

```

2. Write iterative function
```{r}
#create function to make DF, determine cluster centroids, attach centroids to original df and assign label based on closest centroid
#return new dataframe, and then we can repeat this function until convergence
iterate <- function(original_df){
  x_df <- as.data.frame(x)

centroids <- x_df %>%
  group_by(label) %>%
  mutate(mean(input_1), mean(input_2)) %>%
  select(`mean(input_1)`, `mean(input_2)`) %>%
  distinct()
  
x1 <- centroids[1,2]
x2 <- centroids[2,2]
x3 <- centroids[3,2]
y1 <- centroids[1,3]
y2 <- centroids[2,3]
y3 <- centroids[3,3]
xs <- cbind(x1, x2, x3) 
ys <- cbind(y1, y2, y3) 
points <- cbind(xs, ys)
colnames(points) <- c("x1", "x2", "x3", "y1", "y2", "y3")

df_cluster <- as.data.frame(cbind(x_df, points))
  
new_df <- df_cluster %>%
  mutate(label = if_else((((abs(input_1-x1) + abs(input_2-y1)) / 2) < ((abs(input_1-x2) + abs(input_2-y2)) / 2)) & (((abs(input_1-x1) + abs(input_2-y1)) / 2) < ((abs(input_1-x3) + abs(input_2-y3)) / 2)), 1, 
                           if_else((((abs(input_2-x2) + abs(input_2-y2)) / 2) < ((abs(input_1-x3) + abs(input_2-y3)) / 2)), 2, 3))) %>%
  select(input_1, input_2, label)

new_df
}

x2 <- iterate(x)
x == x2
x3 <- iterate(x2)
x2 == x3
```

3. Visualize cluster assignments
```{r}
final_centroids <- as.data.frame(x3) %>%
  group_by(label) %>%
  mutate(mean(input_1), mean(input_2)) %>%
  select(`mean(input_1)`, `mean(input_2)`) %>%
  distinct()

clusters_plot <- ggplot() +
  geom_point(data = as.data.frame(x2), 
             mapping = aes(input_1, input_2, color = as.factor(label))) +
  geom_point(data = final_centroids, 
             mapping = aes(`mean(input_1)`, `mean(input_2)`, color = as.factor(label)), 
             size = 5, 
             alpha = .4)  +
  labs(title = "Cluster Assignments - K = 3")

clusters_plot
```

4. Repeat process for k = 2
```{r}
iterate2 <- function(original_df){
  x_df <- as.data.frame(x)

centroids <- x_df %>%
  group_by(label) %>%
  mutate(mean(input_1), mean(input_2)) %>%
  select(`mean(input_1)`, `mean(input_2)`) %>%
  distinct()
  
x1 <- centroids[1,2]
x2 <- centroids[2,2]
y1 <- centroids[1,3]
y2 <- centroids[2,3]
xs <- cbind(x1, x2) 
ys <- cbind(y1, y2) 
points <- cbind(xs, ys)
colnames(points) <- c("x1", "x2", "y1", "y2")

df_cluster <- as.data.frame(cbind(x_df, points))
  
new_df <- df_cluster %>%
  mutate(label = if_else((((abs(input_1-x1) + abs(input_2-y1)) / 2) < ((abs(input_1-x2) + abs(input_2-y2)) / 2)), 1, 2)) %>%
  select(input_1, input_2, label)

new_df
}

x2b <- iterate2(x)
x == x2b
x3b <- iterate2(x2b)
x2b == x3b

final_centroids2 <- as.data.frame(x3b) %>%
  group_by(label) %>%
  mutate(mean(input_1), mean(input_2)) %>%
  select(`mean(input_1)`, `mean(input_2)`) %>%
  distinct()

ggplot() +
  geom_point(data = as.data.frame(x3b), 
             mapping = aes(input_1, input_2, color = as.factor(label))) +
  geom_point(data = final_centroids2, 
             mapping = aes(`mean(input_1)`, `mean(input_2)`, color = as.factor(label)), 
             size = 5, 
             alpha = .4) +
  labs(title = "Cluster Assignments - K = 2")

clusters_plot

```

5. Discussion

Our initial hunch of 3 clusters did pan out, as the mean distance between points and the cluster centroids is small relative to only using 2 clusters. This is also visually represented in the two plots, where the centroids and groups of data are more intuitively clustered together.

#Application

6. Perform PCA
```{r}
getwd()
wiki <- read_csv("/Users/petecuppernull/Dropbox/UChicago/2019-20/Winter/Computatonal Modeling/Repos/Problem-Set-7-CM/data/wiki.csv")

wiki_fit <- prcomp(wiki, 
                  scale = TRUE)



# visualize
autoplot(wiki_fit,
         shape = TRUE, # names instead of points
         loadings.label = TRUE) + # show the loading directions
theme_bw()

```

The variables that appear strongly correlated with the first component are exp4 (faculty who "contribute to Wikipedia") and faculty who work in engineering and architecture. The second principle component appears to capture direct oopinions aboout the use of Wikipedia for teaching: these avriables include bi1 and bi2 (recommending the use of Wikipedia to colleagues and students and intending to use Wikipedia for teaching in the future), as well as use3 and use4 (currently recommending to colleagues to use Wikipedia and knowing that thheir students currently use it).

7. Calculate PVE 
```{r}
summary(wiki_fit)
```
Approximately 29.1% of the variance is expalined by the first two components.

8. T-SNE
```{r}
wiki_scaled <- scale(wiki)

tsne <- tsne(wiki_scaled, k = 2)

plot(tsne)
```

Here, we see distinct clustering in the data. There appear to be four smaller, well-defined clusters, and one broad loosely correlated cluster. This would suggest that there are certain subsets of faculty which have distinct views on using Wikipedia in the classroom.

